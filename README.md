# ğŸ’¬ Urdu Chatbot - Streamlit Web Application

A professional conversational AI chatbot for Urdu language, powered by a fine-tuned encoder-decoder transformer model. This application provides an intuitive web interface for natural language conversations in Urdu.

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit&logoColor=white)](https://streamlit.io/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white)](https://www.python.org/)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
- [Deployment](#-deployment)
- [Configuration](#-configuration)
- [Examples](#-examples)
- [Troubleshooting](#-troubleshooting)
- [Project Structure](#-project-structure)

---

## ğŸ¯ Overview

This Urdu chatbot is built using state-of-the-art transformer architecture (encoder-decoder) and fine-tuned on conversational data in Urdu. The application provides:

- **Natural Language Understanding** - Processes Urdu text with proper tokenization
- **Context-Aware Responses** - Generates relevant responses based on user input
- **Web Interface** - Clean, modern UI built with Streamlit
- **Session Management** - Maintains conversation history
- **Easy Deployment** - One-click deployment to Streamlit Cloud

---

## âœ¨ Features

### Core Features

âœ… **Auto-Loading Models** - Models load automatically on application startup  
âœ… **Clean Chat Interface** - Modern, user-friendly chat UI with message bubbles  
âœ… **Session History** - Maintains conversation context throughout the session  
âœ… **Example Questions** - Quick access to sample questions in the sidebar  
âœ… **Typing Indicator** - Visual feedback during response generation  
âœ… **Clear Chat Option** - Reset conversation anytime  
âœ… **Responsive Design** - Works seamlessly on desktop and mobile devices

### Technical Features

âœ… **Encoder-Decoder Architecture** - Advanced transformer-based model  
âœ… **SentencePiece Tokenization** - Efficient text processing for Urdu  
âœ… **Automatic GPU/CPU Detection** - Optimizes performance based on available hardware  
âœ… **Caching** - Models cached in memory for fast response times  
âœ… **Error Handling** - Comprehensive error handling and user feedback

---

## ğŸ—ï¸ Architecture

### Model Architecture

The chatbot uses an **Encoder-Decoder Transformer** model with the following specifications:

```
Model Configuration:
â”œâ”€â”€ Embedding Dimension: 512
â”œâ”€â”€ Attention Heads: 4
â”œâ”€â”€ Feed-Forward Dimension: 2048
â”œâ”€â”€ Encoder Layers: 3
â”œâ”€â”€ Decoder Layers: 3
â”œâ”€â”€ Dropout: 0.4
â””â”€â”€ Max Sequence Length: 128
```

### Application Stack

```
Frontend Layer (Streamlit)
    â†“
Inference Layer (inference.py)
    â†“
Model Layer (PyTorch)
    â†“
Tokenizer Layer (SentencePiece)
```

### Workflow

1. **User Input** â†’ Streamlit captures user message
2. **Tokenization** â†’ SentencePiece tokenizes Urdu text
3. **Encoding** â†’ Input encoded as token IDs
4. **Model Inference** â†’ Encoder-decoder processes sequence
5. **Decoding** â†’ Output decoded back to Urdu text
6. **Display** â†’ Response shown in chat interface

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Git (for version control)

### Step 1: Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/urdu-chatbot.git
cd urdu-chatbot
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

**Required packages:**

- `streamlit>=1.28.0` - Web framework
- `torch>=2.0.0` - PyTorch for model
- `sentencepiece>=0.1.99` - Urdu tokenization
- `numpy>=1.24.0` - Numerical operations

### Step 3: Verify Installation

```bash
streamlit --version
python -c "import torch; print(torch.__version__)"
```

---

## ğŸš€ Usage

### Running Locally

1. **Start the Application**

```bash
streamlit run app.py
```

2. **Access the App**

Open your browser and navigate to:

```
http://localhost:8501
```

3. **Start Chatting**

Type your question in Urdu in the chat input box and press Enter.

### Example Interaction

```
You: Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ
Bot: Ù…ÛŒÚº Ù¹Ú¾ÛŒÚ© ÛÙˆÚºØŒ Ø´Ú©Ø±ÛŒÛ! Ø¢Ù¾ Ú©Ø§ Ø¯Ù† Ú©ÛŒØ³Ø§ Ú¯Ø²Ø± Ø±ÛØ§ ÛÛ’ØŸ

You: Ø¢Ù¾ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ
Bot: Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Urdu Chatbot ÛÛ’Û” Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÛŒÛØ§Úº ÛÙˆÚº!
```

---

## â˜ï¸ Deployment

### Streamlit Cloud Deployment

#### Option 1: Deploy Existing GitHub Repository

1. **Push to GitHub**

   ```bash
   git init
   git add .
   git commit -m "Initial commit"
   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git
   git push -u origin main
   ```

2. **Deploy on Streamlit Cloud**
   - Visit https://share.streamlit.io
   - Sign in with GitHub
   - Click **"New app"**
   - Select your repository
   - Set **Main file path**: `app.py`
   - Click **Deploy**

#### Option 2: Deploy from Streamlit Desktop

1. Open Streamlit Desktop app
2. Click **"Deploy"**
3. Follow the prompts to deploy

### Deployment Considerations

**File Size Limits:**

- Streamlit Cloud: 1GB per app
- GitHub: 100MB per file (use Git LFS for larger files)

**Model Files:**

- Ensure `backend/model/` and `backend/tokenizer/` directories are included
- Large model files may require Git LFS

### First Load Performance

- First load: 30-60 seconds (model initialization)
- Subsequent requests: <1 second (cached in memory)

---

## âš™ï¸ Configuration

### Model Configuration

Edit `backend/config.py` to customize:

```python
# Model Architecture
EMBED_DIM = 512          # Embedding dimension
NUM_HEADS = 4             # Attention heads
FF_DIM = 2048            # Feed-forward dimension
ENC_LAYERS = 3           # Encoder layers
DEC_LAYERS = 3           # Decoder layers
DROP_MAX_SEQ_LEN = 128   # Maximum sequence length

# File Paths
TOKENIZER_PATH = "backend/tokenizer/urdu_tokenizer.model"
MODEL_PATH = "backend/model/best_finetuned_epoch30_loss0.3141 (1).pt"

# Generation Settings
DEFAULT_MAX_LENGTH = 100
MAX_GENERATION_LENGTH = 200
```

### Streamlit Configuration

Edit `.streamlit/config.toml`:

```toml
[theme]
primaryColor = "#007bff"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f0f0"
textColor = "#000000"
font = "sans serif"

[server]
headless = true
port = 8501
```

---

## ğŸ’¡ Examples

### Example Questions (Urdu)

Here are some example questions you can try:

| Urdu Question                | Translation               |
| ---------------------------- | ------------------------- |
| Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ                 | How are you?              |
| Ø¢Ù¾ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ            | What is your name?        |
| Ø¢Ø¬ Ù…ÙˆØ³Ù… Ú©ÛŒØ³Ø§ ÛÛ’ØŸ             | How is the weather today? |
| Ú©ÛŒØ§ Ø¢Ù¾ Ù…Ø¬Ú¾Û’ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŸ | Can you help me?          |
| Ø¢Ù¾ Ú©ÛØ§Úº Ø³Û’ Ø¢Ø¦Û’ ÛÛŒÚºØŸ          | Where are you from?       |
| Ø¢Ù¾ Ú©ÛŒØ§ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŸ           | What are you doing?       |
| Ú©ÛŒØ§ Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ø¨ÙˆÙ„ Ø³Ú©ØªÛ’ ÛÛŒÚºØŸ    | Can you speak Urdu?       |
| Ø¢Ù¾ Ú©Ø§ Ø¯Ù† Ú©ÛŒØ³Ø§ Ú¯Ø²Ø±Ø§ØŸ          | How was your day?         |

### Sample Conversation

```
You: Ø¢Ù¾ Ú©Ùˆ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¨Ø§Øª Ú©Ø±Ù†Ø§ Ú©ÛŒØ³Ø§ Ù„Ú¯ØªØ§ ÛÛ’ØŸ
Bot: Ù…Ø¬Ú¾Û’ Ø§Ø±Ø¯Ùˆ Ø²Ø¨Ø§Ù† Ù¾Ø³Ù†Ø¯ ÛÛ’! ÛŒÛ Ø§ÛŒÚ© Ø®ÙˆØ¨ØµÙˆØ±Øª Ø§ÙˆØ± Ø´Ø§Ø¦Ø³ØªÛ Ø²Ø¨Ø§Ù† ÛÛ’Û” Ù…ÛŒÚº Ø§Ø³ Ù…ÛŒÚº Ø¨Ø§Øª Ú©Ø±Ù†Ø§ Ù¾Ø³Ù†Ø¯ Ú©Ø±ØªØ§ ÛÙˆÚºÛ”

You: Ú©ÛŒØ§ Ø¢Ù¾ Ù…Ø¬Ú¾Û’ Ú©ÙˆØ¦ÛŒ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŸ
Bot: Ø¨Ø§Ù„Ú©Ù„! Ù…ÛŒÚº ÛŒÛØ§Úº Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÚºÛ” Ø¢Ù¾ Ú©ÛŒØ§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŸ
```

---

## ğŸ› Troubleshooting

### Common Issues

#### Issue: "Module not found"

**Solution:**

```bash
pip install -r requirements.txt
```

#### Issue: "Tokenizer not found"

**Solution:**

- Verify `urdu_tokenizer.model` exists in `backend/tokenizer/`
- Check path in `backend/config.py`

#### Issue: "Model not found"

**Solution:**

- Verify model file exists in `backend/model/`
- Check filename matches exactly in `backend/config.py`

#### Issue: "Port already in use"

**Solution:**

```bash
streamlit run app.py --server.port 8502
```

#### Issue: "Out of memory"

**Solution:**

- Reduce `MAX_SEQ_LEN` in config
- Close other applications
- Use cloud GPU instance

#### Issue: Model loads slowly

**Solution:**

- Normal for first load (30-60 seconds)
- Models are cached after first load
- Consider model optimization/quantization

### Clear Streamlit Cache

```bash
streamlit cache clear
```

---

## ğŸ“ Project Structure

```
urdu-chatbot/
â”‚
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”‚
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml                 # Streamlit configuration
â”‚
â””â”€â”€ backend/
    â”œâ”€â”€ config.py                  # Model & path configuration
    â”œâ”€â”€ inference.py               # Model inference logic
    â”‚
    â”œâ”€â”€ tokenizer/
    â”‚   â””â”€â”€ urdu_tokenizer.model   # SentencePiece tokenizer
    â”‚
    â””â”€â”€ model/
        â””â”€â”€ best_finetuned_epoch30_loss0.3141 (1).pt  # Trained model
```

### Key Files Explained

| File                     | Purpose                                                 |
| ------------------------ | ------------------------------------------------------- |
| `app.py`                 | Streamlit web application entry point                   |
| `backend/inference.py`   | Model loading, tokenization, and text generation        |
| `backend/config.py`      | Configuration for paths, model parameters, and settings |
| `.streamlit/config.toml` | Streamlit UI theme and server settings                  |
| `requirements.txt`       | Python package dependencies                             |

---

## ğŸ“ Technical Details

### Model Training

The model was fine-tuned using:

- **Architecture**: Encoder-Decoder Transformer
- **Task**: Conversational response generation
- **Language**: Urdu
- **Training**: Fine-tuned for 30 epochs
- **Loss**: 0.3141 (final model)

### Tokenization

- **Method**: SentencePiece
- **Vocabulary Size**: Model-specific (from tokenizer)
- **Special Tokens**: `<pad>`, `<s>`, `</s>` (BOS/EOS)

### Generation Process

1. Input question â†’ Encode with BOS token
2. Pass through encoder layers
3. Generate tokens sequentially with decoder
4. Stop at EOS token
5. Decode tokens â†’ Urdu response

---

## ğŸ“„ License

This project is provided as-is for educational and research purposes.

---

## ğŸ‘¥ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“ Support

For issues, questions, or suggestions:

- Open an issue on GitHub
- Check the troubleshooting section above
- Review project documentation

---

## ğŸ™ Acknowledgments

- Built with [Streamlit](https://streamlit.io/)
- Powered by [PyTorch](https://pytorch.org/)
- Tokenization by [SentencePiece](https://github.com/google/sentencepiece)

---

**Made with â¤ï¸ for the Urdu language community**
